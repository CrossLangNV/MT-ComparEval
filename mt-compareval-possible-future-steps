

MT-Compareval possible future steps
===================================

1. EXPORT
---------

Some export possibilities have already been implemented.
Further possibilities:
- export post editing information along with the sentences (the discussion about storing post-editing information in the xliff format, and the format specifications for that, is currently ongoing)
- export tables and figures to csv, LaTeX, pdf...
- ...

3. MULTIPLE TEST SET COMPARISON
-------------------------------

This is already implemented; possible improvements:
- measure how well an engine does for a domain
- add a metric for how significat a certain score is
- currently, the comparisons are based upon BLEU scores. Make this configurable so that other metrics can be used for comparison as well.
- make comparison of a subset of test sets possible (currently: all test sets / per domain)


4. POSSIBILITY TO UPLOAD METRIC SCORES
--------------------------------------

Possibility to import tasks with already computed metric scores (instead of computing the scores by MT-ComparEval).

5. IMPROVEMENT OF GLOBAL ENGINE HIERARCHY
-----------------------------------------

UX can be improved.

6. ADDITIONAL METRICS
---------------------

Provide wrappers for popular existing metrics like sacreBLEU and multeval.

7. PROVIDE SOME SAMPLE DATA FOR TESTING
---------------------------------------

E.g.: with sacreBLEU you can download WMT test sets. Provide a similar possibility in the application.

8. SQLITE ALTERNATIVE
---------------------

Research alternatives to improve performance.

9. MULTI-USER ENVIRONMENT
-------------------------

Implement a signup/login system. Each user has their own data.

10. MAKE THE ENGINE TRAINING FILES VISIBLE/DOWNLOADABLE FROM THE INTERFACE
--------------------------------------------------------------------------

11. A PER-SENTENCE COMPARISON VIEW
----------------------------------

Add a view which would allow to compare translations of one sentence by different engines (more than two engines, which is currently possible in the Task view).

12. ADD BLIND EVALUATION VIEW
-----------------------------

An evaluator evaluates translations by different engines. He does not see which translation comes from which engine.

13. GRAPH VISUALIZATION IN PYTHON
---------------------------------

Note: Joachim is currently working on this.

14. NGRAMS GENERATION FROM PYTHON
---------------------------------

Note: on Anna's todo list.

